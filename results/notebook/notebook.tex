\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes,positioning}
\usepackage{multicol}
\usepackage{listings}



%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Notebook}
\author{Fouad A.I. Azar}
\date{\today}

\begin{document}
\maketitle

\section*{Activities for 09-09-2023}
\subsection*{Creation of a Bash Script}
A new bash script was developed for the creation of a root directory to house the standard bioinformatics subdirectories. These subdirectories include \textit{bin}, \textit{src}, \textit{results}, \textit{doc}, and \textit{data}.

\subsection*{Development of a Python Script}
A Python script was written with the primary function of sending prompts to OpenAI. These prompts are mainly in relation to code updates and potential research ideas. Notably, the responses from these prompts together with the prompts themselves are stored in a PSQL database and individual text files. This practice ensures that the GPT information is meticulously recorded, providing a digital timeline of when, why and how the data was utilized.

\subsection*{Data Storage}
Photospectroscopic data and SMILES (Simplified Molecular Input Line Entry System) of chemical compounds were gathered into a CSV file named \textit{molecules.csv}. This file contains estimated data of about 25,000 rows and was stored in the \textit{data} subdirectory labelled with today's date.

\subsection*{Utilization of a Padel-Descriptor Python Script}
A Python script using a Padel-Descriptor tool is currently running to extract around 2,000 fields of data for the purpose of neural network training. There is an ongoing consideration to relocate this script into Mojo.

\section*{Acitivities for 10-09-2023}
\subsection*{Molecule Data Processing}
After running \texttt{padel.py} overnight, \texttt{2000 lines} of \texttt{molecules.csv} were processed, but not all were successfully interpreted due to \textit{run time errors}. To remedy this issue, an instance of \texttt{EC2} in \texttt{AWS} was created. The associated \texttt{.pem} file is located in the \texttt{bin} directory together with an executable bash script that opens the \texttt{EC2} instance named \texttt{Flurine}.

\subsection*{Mojo Exploration}
Performed some experimental explorations with \texttt{Mojo}. Since it lacks a list functionality, creating a neural network from scratch seemed more to be an exercise in futility than a scientific endeavor. 

\subsection*{Neural Network Class}
Developed a simple, modular neural network class in Python. However, with a million epochs it's failing to train as an \textit{exor}. It prompts the question of whether to allow it to run for longer.  

\subsection*{Master Thesis}
The master thesis was successfully downloaded and stored under \newline
\verb|~/MA/results/thesis/legacy|.\newline 
Utilizing \texttt{pdflatex} and \texttt{bibtex} to execute it produced no errors.

\section*{Activities for 11-09-2023}
Firstly, today I worked on writing Python scripts with the purpose of creating PSQL table for a file named \texttt{"descriptors.csv"}. During this task, I encountered a couple of challenges. The first problem arose when the number of descriptors calculated surpassed the limit of 1600 columns that PSQL can handle. 

The second issue was with the headers generated by Padel. These headers were not suitable for use in PSQL as they contained prohibited characters such as "-" and reserved terms like "As". 

To tackle these challenges, I developed two scripts. \texttt{sanitize\_headers.py} This script was created to sanitize the headers and make them usable in PSQL. \texttt{appendHeaders.py}: the purpose of this script is suggested by its name - it was designed to append headers. 

\section*{Activities for 12-09-2023}
\subsection*{Giving up on exporting to psql}
I initially considered using a psql table to organize my data, but the limitations in terms of column numbers and the sensitive nature of psql will result in a larger time investment than what is optimal for the project. Thus, I've decided to continue using .csv files instead.

\subsection*{Testing the Neural Network}
The neural network I coded was tested extensively. However, even after 100,000 epochs, the data is not properly converging. Two potential causes have been identified: 

\subsubsection*{Data Normalization} 
The data points may not be properly normalized or standardized, given that they are real numbers with no presumed fixed range. Further research will be conducted to understand how to improve data normalization for this specific case and how the PaDEL descriptor calculates, which might influence decision on normalization.

\subsubsection*{Architecture and Activation Functions}
There could be faults in the architecture and choice of activation functions. The current setup is [len(x),1024][1024,512][512,256][256.128][128,1], with each layer using a Rectified Linear Unit (ReLU) as its activation function. Further investigation is required to identify the most suitable architecture and activation functions for this type of application.

\subsection*{Data Distribution}
Histograms were created to observe the distribution of emission maxima and molecular weights. However, I will be redoing all of this again when I have the complete dataset.

\section*{Activities for 13-09-2023}
\subsection*{Data Generation Pipeline}
I have successfully created a pipeline for generating our training data. This simplifies the process, obviating the need for multiple scripts to prepare the data. The pipeline operates as follows:

\begin{enumerate}
    \item Extract descriptors without fingerprints from \texttt{molecules.csv}. This process is separated due to the intensive computational need which might take days using a simple laptop without a GPU. This creates a new file named \texttt{descriptors.csv}, which will be automatically duplicated into \texttt{descriptors\_backup.csv}. The current copy is located at \texttt{\~/MA/data/2023-09-10/}, while the \texttt{molecules.csv} is at \texttt{\~/MA/data/2023-09-09}.
    
    \item Append headers to \texttt{descriptor\_backup.csv}. This is due to a flaw with the \texttt{to\_csv} function from the library padelpy that doesn't export CSV files with headers. To execute this, you'll need \texttt{headers.csv} in the same directory as the script.
    
    \item Match the tags between the descriptors and molecules to identify which rows have been processed. This can be problematic because of padelpy's \texttt{from\_smiles} function's long runtime. My \texttt{padel.py} script has a try and catch so that the extraction for each smile goes uninterrupted. The output CSV will contain all the rows from descriptors plus the target "Emission max (nm)" from \texttt{molecules.csv}.
    
    \item Lastly, apply a mask to remove all NaN's. This is important in case there is a target output with no value. 
\end{enumerate}

All of these processes are located under \texttt{\~/MA/src/pipeline/main.py}.

\subsection*{Neural Network Training}
I have completed the first successful neural network training process. I used tensorflow to test if my hypothesis would yield a positive correlation between the predicted and experimental values of the emission maximum.

\subsection*{Histograms}
Finally, I created some histograms to visualise the distribution of emission maxima and molecular weight. However, these need to be updated again once the EC2 instance is done calculating the PaDEL descriptors.

\section*{Activities of 16-09-2023}
This is the first section that I will be writting informally without the use of chatgpt. I had a meeting with good old Professor Jameson. In our conversation he was very impressed with the fact that my AI waws able to accurately predict the emission maxima. We tried talking about applications for this, and one of the few things he mentioned are:
\begin{itemize}
	\item If this kind of algorithm could predict photostability. I will need to do some research on how photostability is quantifiable at all
	\item The application of this in probe (fluorophore) design would be perfec. Maybe the ability to reverse the process and provide the so called X's to the desired Y's. This kind of retrosythentic calculation may be difficult to conduct since we might have a many to one kind of sitation when we map Y $\rightarrow$ X.
	\item Plotting the fluorescence spectra in accodirace with the solvents polarity? Adding some kind of elaborative explaination how the eviornment aka solvent affects the spectrum. My best bet is to probably split up the database into different solvents. amd train each network separately given the solvent might be smarter than training one database containing varying solvents.
\end{itemize}
One thing I would like to test today is if in general predictions can be made on the other photometric phenomena. 

\section{Activities 2023-10-02}
Professor David Jameson suggests addressing the issue of effects on the fluorescence spectrum. He implies that a machine learning algorithm detecting the fluorescence spectrum should consider utilizing the Padel descriptors of the solvent to fully determine the resulting fluorescence spectrum.

I will conduct a test today to see if increasing the input of the feature vector to include a sort of "bias" for the solvent could make sense?

I will run the NN model with a so-called additive method, where the padel descriptors are added to each other.

\section{Activities 2023-10-03}
My goal is to develop a system that identifies if a polypeptide chain contains aromatic amino acids to predict fluorescence. The process utilizes software packages to analyze these chains quantitatively, possibly qualitatively. Predictions on fluorescence emission maximum are made using a Multi-Layer Perceptron (MLP). I hypothesize that while the excitation spectrum remains constant, the excitation maximum could vary due to potential quenching factors. I am aware of the FPBase database, but need more samples to test my theory effectively.

I'm still calculating the descriptors for everything, but to be honest I think I am to blame for why this process is taking so long. PadelPy is great on a small scale, but when you want to calculate descriptors for 10,000s of smile files, a better solution is needed.


\section{Activities 2023-10-23}
I plan to focus on enhancing my data collection methods due to the current frustrating system of organization within the data and results folder. They also intend to address issues with the Neural Network (NN) script, possibly by redesigning it from its foundation, to improve its operation.

\section{Activities 2023-10-27}
I have made a new folder named 'main' under 'data and results' to store main files. They modified the 'neuralNetwork2.py' script for optimized mlp-learning. For the script to function effectively, they removed erroneous rows (8500 to 8504) from the 'train.csv' file, hence eradicating incomplete and faulty data.

\subsection{Proteins}

The provided Python script fetches protein data from the \href{https://www.fpbase.org/}{FPbase} using its GraphQL endpoint. Specifically, it retrieves information about various proteins and their respective states. Once fetched, this data is saved locally as a JSON file.

\subsection{Functionality}

The script performs the following steps:

\begin{enumerate}
    \item Defines a GraphQL query to fetch protein-related details such as ID, sequence, pdb, name, and states.
    \item Sends a POST request to the FPbase GraphQL endpoint with the constructed query.
    \item Upon a successful response, the fetched data is saved to a file named \texttt{proteins.json}.
    \item If the request fails, an error message with the respective status code is displayed.
\end{enumerate}

\subsection{Implementation}

Here's a brief snippet of the code:

\begin{lstlisting}[language=Python]
import requests
import json

GRAPHQL_URL = "https://www.fpbase.org/graphql/"
# ... [GraphQL query definition] ...

response = requests.post(GRAPHQL_URL, data=json.dumps({"query": graphql_query}), headers=headers)

if response.status_code == 200:
    with open('proteins.json', 'w') as file:
        json.dump(data, file, indent=4)
    print("Data saved to proteins.json!")
else:
    print("Failed to fetch data. Status code:", response.status_code)
\end{lstlisting}

In essence, the script serves as a bridge between the user and the FPbase database, simplifying the process of data retrieval and storage.

\subsection{what now}
The next steps will now have to be focusing on structering the data for machine learning. We will feed the the sequence into some protein structure descriptor and quantifier, and have it mapped to the intially just the emission spectrum.


\section{Activities 2023-12-01}
Today's objective centered around enhancing the FPBase data pipeline for analysis. Given my month-long hiatus from this project, I invested time today to refresh my understanding of the code. I successfully streamlined and optimized both the NN.py and evaluation.py code. The findings from this study, situated under src/FPBase/figures/, show promising results. Tomorrow, my efforts will pivot towards increasing the parameters in the NN. Presently, it relies solely on the descriptors procured from rcpi's extractPAAC. While it offers a decent number of parameters, I am convinced that a more comprehensive insight into the protein structure is necessary for the NN to predict each protein's spectra accurately. Consequently, I will review the documentation tomorrow, selecting the most suitable functions from RCPI.
\end{document}
